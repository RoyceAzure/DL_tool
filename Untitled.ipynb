{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir coco/ && mkdir coco/images && mkdir coco/annotations\n",
    "\n",
    "!git clone https://github.com/cocodataset/cocoapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_DataSet_url = \"http://images.cocodataset.org/zips/train2014.zip\"\n",
    "annotations_url = \"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\"\n",
    "\n",
    "image_dir = \"coco/images\"\n",
    "annotation_dir = \"coco/annotations\"\n",
    "\n",
    "annotation_file = annotation_dir + \"/\" + os.path.basename(annotations_url)\n",
    "image_file = image_dir + \"/\" + os.path.basename(coco_DataSet_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download(coco_2017DataSet_url, image_dir)\n",
    "\n",
    "# download(annotations_trainval2014_url, annotation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_ref = zipfile.ZipFile(annotation_file, 'r')\n",
    "zip_ref.extractall(annotation_dir)\n",
    "zip_ref.close()\n",
    "\n",
    "zip_ref = zipfile.ZipFile(image_file, 'r')\n",
    "zip_ref.extractall(image_dir)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def muti_download_process(url, target_dir):\n",
    "    download(url, target_dir)\n",
    "    zip_ref = zipfile.ZipFile(target_dir + \"/\" + os.path.basename(url), 'r')\n",
    "    zip_ref.extractall(target_dir)\n",
    "    zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = []\n",
    "threads.append(threading.Thread(target = muti_download_process, args = (coco_DataSet_url,image_dir)))\n",
    "threads.append(threading.Thread(target = muti_download_process, args = (annotations_url,annotation_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in threads:\n",
    "    i.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in threads:\n",
    "    i.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls coco/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls coco/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_json_path = \"coco/annotations/annotations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\"\"\"\n",
    "instances person_keypoints 都有 categories 這個key\n",
    "captions沒有\n",
    "\"\"\"\n",
    "\n",
    "with open(\"coco/annotations/annotations/captions_train2014.json\") as f:\n",
    "    captions_train2014 = json.load(f)\n",
    "with open(\"coco/annotations/annotations/captions_val2014.json\") as f:\n",
    "    captions_val2014 = json.load(f)\n",
    "with open(\"coco/annotations/annotations/instances_train2014.json\") as f:\n",
    "    instances_train2014 = json.load(f)\n",
    "with open(\"coco/annotations/annotations/instances_val2014.json\") as f:\n",
    "    instances_val2014= json.load(f)\n",
    "with open(\"coco/annotations/annotations/person_keypoints_train2014.json\") as f:\n",
    "    person_keypoints_train2014 = json.load(f)\n",
    "with open(\"coco/annotations/annotations/person_keypoints_val2014.json\") as f:\n",
    "    person_keypoints_val2014 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "captions_train2014[\"annotations\"] : list of \n",
    "{'image_id': 318556,\n",
    "  'id': 48,\n",
    "  'caption': 'A very clean and well decorated empty bathroom'},\n",
    "  \n",
    "captions_train2014[\"licenses\"] 8個license id 還有出處\n",
    "\n",
    "info : 整體資料及資訊 \n",
    "{'description': 'COCO 2014 Dataset',\n",
    " 'url': 'http://cocodataset.org',\n",
    " 'version': '1.0',\n",
    " 'year': 2014,\n",
    " 'contributor': 'COCO Consortium',\n",
    " 'date_created': '2017/09/01'}\n",
    " \n",
    " images :\n",
    " \n",
    "  {'license': 5,\n",
    "  'file_name': 'COCO_train2014_000000057870.jpg',\n",
    "  'coco_url': 'http://images.cocodataset.org/train2014/COCO_train2014_000000057870.jpg',\n",
    "  'height': 480,\n",
    "  'width': 640,\n",
    "  'date_captured': '2013-11-14 16:28:13',\n",
    "  'flickr_url': 'http://farm4.staticflickr.com/3153/2970773875_164f0c0b83_z.jpg',\n",
    "  'id': 57870},\n",
    "\"\"\"\n",
    "captions_train2014[\"images\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 讀取api 裡面相關的 label_map.pbtxt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "看來 api 裡面已經有存一些資料的 label map\n",
    " 適用 id : {\n",
    "     id: \n",
    "     name:\n",
    " }\n",
    " 使用abel_map_util.create_category_index_from_labelmap  可直接讀近來轉成 dict\n",
    "\"\"\"\n",
    "PATH_TO_LABELS = './models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 抓model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "handel 已經用 list 處理好\n",
    "\"\"\"\n",
    "model_display_name = 'CenterNet HourGlass104 Keypoints 512x512' # @param ['CenterNet HourGlass104 512x512','CenterNet HourGlass104 Keypoints 512x512','CenterNet HourGlass104 1024x1024','CenterNet HourGlass104 Keypoints 1024x1024','CenterNet Resnet50 V1 FPN 512x512','CenterNet Resnet50 V1 FPN Keypoints 512x512','CenterNet Resnet101 V1 FPN 512x512','CenterNet Resnet50 V2 512x512','CenterNet Resnet50 V2 Keypoints 512x512','EfficientDet D0 512x512','EfficientDet D1 640x640','EfficientDet D2 768x768','EfficientDet D3 896x896','EfficientDet D4 1024x1024','EfficientDet D5 1280x1280','EfficientDet D6 1280x1280','EfficientDet D7 1536x1536','SSD MobileNet v2 320x320','SSD MobileNet V1 FPN 640x640','SSD MobileNet V2 FPNLite 320x320','SSD MobileNet V2 FPNLite 640x640','SSD ResNet50 V1 FPN 640x640 (RetinaNet50)','SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)','SSD ResNet101 V1 FPN 640x640 (RetinaNet101)','SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)','SSD ResNet152 V1 FPN 640x640 (RetinaNet152)','SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)','Faster R-CNN ResNet50 V1 640x640','Faster R-CNN ResNet50 V1 1024x1024','Faster R-CNN ResNet50 V1 800x1333','Faster R-CNN ResNet101 V1 640x640','Faster R-CNN ResNet101 V1 1024x1024','Faster R-CNN ResNet101 V1 800x1333','Faster R-CNN ResNet152 V1 640x640','Faster R-CNN ResNet152 V1 1024x1024','Faster R-CNN ResNet152 V1 800x1333','Faster R-CNN Inception ResNet V2 640x640','Faster R-CNN Inception ResNet V2 1024x1024','Mask R-CNN Inception ResNet V2 1024x1024']\n",
    "model_handle = ALL_MODELS[model_display_name]\n",
    "\n",
    "\"\"\"\n",
    "直接load (網址就可以)  她媽的方便\n",
    "\"\"\"\n",
    "print('loading model...')\n",
    "hub_model = tf.keras.Sequential([hub.KerasLayer(\n",
    "    model_handle, \n",
    "    input_shape=[512,512,3],     # Expects a tensor of shape [batch_size] as input.\n",
    "    dtype=tf.uint8)])    # Expects a tf.string input tensor.\n",
    "print('model loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Image Selection (don't forget to execute the cell!) { display-mode: \"form\"}\n",
    "selected_image = 'Beach' # @param ['Beach', 'Dogs', 'Naxos Taverna', 'Beatles', 'Phones', 'Birds']\n",
    "flip_image_horizontally = False #@param {type:\"boolean\"}\n",
    "convert_image_to_grayscale = False #@param {type:\"boolean\"}\n",
    "\n",
    "image_path = IMAGES_FOR_TEST[selected_image]\n",
    "\"\"\"\n",
    "load_image_into_numpy_array 是給網址抓圖片  只抓一張  轉成np 並處理維度\n",
    "算是好用\n",
    "\"\"\"\n",
    "image_np = load_image_into_numpy_array(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip horizontally\n",
    "if(flip_image_horizontally):\n",
    "    image_np[0] = np.fliplr(image_np[0]).copy()\n",
    "\n",
    "# Convert image to grayscale\n",
    "\"\"\"\n",
    "注意這裡轉灰階的方式\n",
    "numpy.fliplr(m)[source]\n",
    "    Flip array in the left/right direction.\n",
    "\n",
    "    Flip the entries in each row in the left/right direction. Columns are preserved, but appear in a different order than before.\n",
    "    \n",
    "numpy.tile(A, reps)[source]\n",
    "    Construct an array by repeating A the number of times given by reps.\n",
    "\"\"\"\n",
    "if(convert_image_to_grayscale):\n",
    "    image_np[0] = np.tile(\n",
    "    np.mean(image_np[0], 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(24,32))\n",
    "plt.imshow(image_np[0])\n",
    "plt.show()\n",
    "\n",
    "# running inference\n",
    "results = hub_model(image_np)\n",
    "\n",
    "# different object detection models have additional results\n",
    "# all of them are explained in the documentation\n",
    "result = {key:value.numpy() for key,value in results.items()}\n",
    "print(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "hub_url = \"https://tfhub.dev/google/nnlm-en-dim128/2\"\n",
    "embed = hub.KerasLayer(hub_url,trainable=True)\n",
    "embeddings = embed([\"A long sentence.\", \"single-word\", \"http://example.com\"])\n",
    "print(embeddings.shape, embeddings.dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
